import requests
import threading
import json
import os
import time
import random
import re
import base64
import yaml
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, parse_qs, urlencode, quote
import logging
from concurrent.futures import ThreadPoolExecutor
import hashlib
import uuid

# ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

# æ¸…ç©ºæ§åˆ¶å°
os.system('cls' if os.name == 'nt' else 'clear')

# --- é…ç½®æ–‡ä»¶ ---
CONFIG_TG_TXT_FILE = 'configtg.txt'
CONFIG_TG_YAML_FILE = 'configtg.yaml'
TG_CHANNELS_FILE = 'telegramchannels.json'
INV_TG_CHANNELS_FILE = 'invalidtelegramchannels.json'

# --- å…¨å±€æ•°æ®å’Œé” ---
all_potential_links_data = []
data_lock = threading.Lock()
sem_pars = None

# --- ç”¨æˆ·ä»£ç†åˆ—è¡¨ ---
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
    'Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Mobile Safari/537.36',
]

# --- éå…³é”®æŸ¥è¯¢å‚æ•°ï¼ˆç”¨äºå»é‡ï¼‰ ---
# è¿™äº›å‚æ•°çš„å˜åŠ¨é€šå¸¸ä¸å½±å“èŠ‚ç‚¹åŠŸèƒ½ï¼Œå› æ­¤åœ¨å»é‡æ—¶å¯ä»¥å¿½ç•¥
NON_CRITICAL_QUERY_PARAMS = {'ed', 'fp', 'allowInsecure', 'obfsParam', 'protoparam', 'ps', 'sid'}

# --- è¾…åŠ©å‡½æ•° ---

def json_load(path):
    """åŠ è½½ JSON æ–‡ä»¶"""
    if not os.path.exists(path):
        logging.info(f"æ–‡ä»¶ {path} ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚")
        return []
    try:
        with open(path, 'r', encoding="utf-8") as file:
            content = file.read()
            if not content:
                logging.warning(f"æ–‡ä»¶ {path} å†…å®¹ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚")
                return []
            data = json.loads(content)
            logging.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶ {path}ã€‚")
            return data
    except json.JSONDecodeError:
        logging.warning(f"æ–‡ä»¶ {path} å†…å®¹æ— æ•ˆ (JSON æ ¼å¼é”™è¯¯)ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚")
        return []
    except Exception as e:
        logging.error(f"åŠ è½½æ–‡ä»¶ {path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

def json_dump(data, path):
    """ä¿å­˜ JSON æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        with open(path, 'w', encoding="utf-8") as file:
            json.dump(data, file, indent=4, ensure_ascii=False)
        logging.info(f"æˆåŠŸä¿å­˜æ–‡ä»¶ {path}ã€‚")
    except Exception as e:
        logging.error(f"ä¿å­˜æ–‡ä»¶ {path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def write_lines(data, path):
    """å°†å­—ç¬¦ä¸²åˆ—è¡¨å†™å…¥æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ª"""
    try:
        with open(path, "w", encoding="utf-8") as file:
            for item in data:
                file.write(str(item) + "\n")
        logging.info(f"æˆåŠŸä¿å­˜æ–‡ä»¶ {path}ã€‚")
    except Exception as e:
        logging.error(f"ä¿å­˜æ–‡ä»¶ {path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def yaml_dump(data, path):
    """ä¿å­˜ YAML æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        with open(path, 'w', encoding="utf-8") as file:
            yaml.dump(data, file, allow_unicode=True, sort_keys=False)
        logging.info(f"æˆåŠŸä¿å­˜æ–‡ä»¶ {path}ã€‚")
    except Exception as e:
        logging.error(f"ä¿å­˜æ–‡ä»¶ {path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def clean_and_urldecode(raw_string):
    """æ¸…ç†å¹¶è§£ç  URL å­—ç¬¦ä¸²"""
    if not isinstance(raw_string, str):
        return None

    cleaned = raw_string.strip()
    cleaned = re.sub(r'amp;', '', cleaned)
    # ç§»é™¤ URL ç¼–ç çš„æ¢è¡Œç¬¦å’Œç©ºå­—èŠ‚
    cleaned = re.sub(r'%0A|%250A|%0D|\\n|%00', '', cleaned, flags=re.IGNORECASE)

    decoded = cleaned
    max_decode_attempts = 5
    for _ in range(max_decode_attempts):
        try:
            new_decoded = requests.utils.unquote(decoded)
            if new_decoded == decoded:
                break
            decoded = new_decoded
        except Exception as e:
            logging.debug(f"URL è§£ç å¤±è´¥: {decoded[:50]}... é”™è¯¯: {e}")
            break

    return decoded

def is_valid_link(parsed_url):
    """éªŒè¯è§£æåçš„ URL æ˜¯å¦æœ‰æ•ˆ"""
    valid_schemes = ['vmess', 'vless', 'ss', 'trojan', 'tuic', 'hysteria', 'hysteria2', 'hy2', 'juicity', 'nekoray', 'socks4', 'socks5', 'socks', 'naive+', 'ssr']
    if parsed_url.scheme not in valid_schemes:
        logging.debug(f"é“¾æ¥åè®®ä¸å—æ”¯æŒ: {parsed_url.scheme}")
        return False

    if not parsed_url.hostname or '.' not in parsed_url.hostname:
        logging.debug(f"é“¾æ¥ä¸»æœºåæ— æ•ˆ: {parsed_url.hostname}")
        return False

    if parsed_url.port is None or not (1 <= parsed_url.port <= 65535):
        logging.debug(f"é“¾æ¥ç«¯å£æ— æ•ˆ: {parsed_url.port}")
        return False

    return True

def is_uuid(s):
    """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ UUID"""
    try:
        uuid.UUID(s)
        return True
    except ValueError:
        return False

def normalize_repeated_patterns(text):
    """
    è§„èŒƒåŒ–é‡å¤çš„æ¨¡å¼ï¼Œä¾‹å¦‚ 'A-A-A' å˜æˆ 'A'ï¼Œæˆ– 'Telegram:@NUFiLTER-Telegram:@NUFiLTER'
    ä¹Ÿä¼šå°è¯•å»é™¤å¸¸è§çš„å¹¿å‘Šè¯æˆ–å¤šä½™çš„çŸ­è¯­ï¼Œå¹¶ç¡®ä¿é•¿åº¦ä¸ä¼šè¿‡é•¿
    """
    if not isinstance(text, str):
        return text
    
    # ç§»é™¤ URL ç¼–ç çš„ @ å’Œ :
    text = re.sub(r'%40', '@', text, flags=re.IGNORECASE)
    text = re.sub(r'%3A', ':', text, flags=re.IGNORECASE)

    # é’ˆå¯¹ Telegram é¢‘é“åæ¨¡å¼è¿›è¡Œè§„èŒƒåŒ–
    # ä¾‹å¦‚: Telegram:@NUFiLTER-Telegram:@NUFiLTER-Telegram:@NUFiLTER -> Telegram:@NUFiLTER
    # ä½¿ç”¨éè´ªå©ªåŒ¹é…å’Œé‡å¤ç»„æ¥å¤„ç†å¤šé‡é‡å¤
    text = re.sub(r'((?:Telegram:)?@[a-zA-Z0-9_]+)(?:-\1)+', r'\1', text, flags=re.IGNORECASE)
    
    # é’ˆå¯¹ ZEDMODEON è¿™æ ·çš„æ¨¡å¼è¿›è¡Œè§„èŒƒåŒ–
    # ä¾‹å¦‚: ZEDMODEON-ZEDMODEON-ZEDMODEON -> ZEDMODEON
    # å¢åŠ å•è¯è¾¹ç•Œï¼Œé¿å…è¯¯ä¼¤
    text = re.sub(r'([a-zA-Z0-9_]+)(?:-\1)+', r'\1', text, flags=re.IGNORECASE)

    # ç§»é™¤å¤šä½™çš„è¿å­—ç¬¦ï¼ˆå¦‚æœè§„èŒƒåŒ–åå‡ºç° A--B è¿™æ ·çš„æƒ…å†µï¼‰
    text = re.sub(r'-+', '-', text)
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„è¿å­—ç¬¦
    text = text.strip('-')

    # ç§»é™¤å¸¸è§çš„å¹¿å‘Šæˆ–ä¸ç›¸å…³çŸ­è¯­
    text = re.sub(r'(?:Join--VPNCUSTOMIZE\.V2ray\.re)', '', text, flags=re.IGNORECASE)
    text = re.sub(r'(?:telegram:)?@?[a-zA-Z0-9_]+(?:-|$)','',text,flags=re.IGNORECASE) # ç§»é™¤ç‹¬ç«‹çš„é¢‘é“åï¼Œå¦‚æœå®ƒä»¬åªæ˜¯å™ªéŸ³

    # å†æ¬¡æ¸…ç†å¤šä½™è¿å­—ç¬¦å’Œæ–œæ 
    text = re.sub(r'-+', '-', text).strip('-')
    text = re.sub(r'/+', '/', text).strip('/')

    # å¦‚æœå¤„ç†åä»ç„¶éå¸¸é•¿ï¼Œå¯ä»¥è€ƒè™‘æˆªæ–­æˆ–å“ˆå¸Œï¼Œä½†éœ€è°¨æ…
    # ç›®å‰ä¸å¼ºåˆ¶æˆªæ–­ï¼Œå› ä¸ºå¯èƒ½ä¸¢å¤±ä¿¡æ¯
    return text

def normalize_domain(domain):
    """
    è§„èŒƒåŒ–åŸŸåï¼Œç»Ÿä¸€å°å†™ï¼Œç§»é™¤ www. å‰ç¼€ï¼Œå¹¶å°è¯•å¤„ç† vXX. æ¨¡å¼
    ä¾‹å¦‚ï¼šv16.vxlimir.com -> vxlimir.com
    """
    if not isinstance(domain, str):
        return domain
    normalized = domain.lower()
    if normalized.startswith('www.'):
        normalized = normalized[4:]
    
    # å°è¯•ç§»é™¤ vXX. æ¨¡å¼ (ä¾‹å¦‚ v16.example.com -> example.com)
    # é¿å…è¯¯ä¼¤åˆæ³•å­åŸŸåï¼Œåªé’ˆå¯¹ v+æ•°å­—.
    # ä¹Ÿä¼šå°è¯•å¤„ç†ç±»ä¼¼ "v2ray-fark2.ddns.net" -> "v2ray-fark.ddns.net" è¿™ç§å¯èƒ½
    normalized = re.sub(r'^v\d+\.', '', normalized)
    normalized = re.sub(r'-fark\d+\.', '-fark.', normalized) # é’ˆå¯¹ v2ray-farkN.ddns.net

    # ç§»é™¤æœ«å°¾çš„ . ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    normalized = normalized.strip('.')

    return normalized

def normalize_path(path):
    """è§„èŒƒåŒ–è·¯å¾„ï¼Œç§»é™¤é‡å¤çš„é¢‘é“åç§°ï¼Œæ¸…ç†æ–œæ ï¼Œå¹¶å»é™¤è·¯å¾„ä¸­çš„æŸ¥è¯¢å‚æ•°"""
    if not path:
        return ''
    
    # åˆ†ç¦»è·¯å¾„å’Œå¯èƒ½çš„æŸ¥è¯¢å‚æ•°
    path_without_query = path.split('?', 1)[0].split('#', 1)[0]
    
    normalized_path = normalize_repeated_patterns(path_without_query)
    # ç§»é™¤è·¯å¾„æœ«å°¾çš„æ–œæ ï¼Œé™¤éæ˜¯æ ¹è·¯å¾„
    return normalized_path.strip('/')

def normalize_remark(remark_text):
    """
    è§„èŒƒåŒ–èŠ‚ç‚¹å¤‡æ³¨ï¼Œå»é™¤å¸¸è§å™ªéŸ³ã€å¹¿å‘Šè¯ã€åºå·ã€æ—¶é—´æˆ³ã€å›½æ——ã€éšæœºå­—ç¬¦ã€ä¼ è¾“åè®®ç­‰ï¼Œ
    ä»¥æé«˜å»é‡å‡†ç¡®æ€§ã€‚
    """
    if not isinstance(remark_text, str):
        return ''
    
    normalized = remark_text.lower()
    
    # ç§»é™¤å¸¸è§çš„é¢‘é“åå’Œå¹¿å‘Šè¯
    normalized = re.sub(r'(@[a-zA-Z0-9_]+)', '', normalized) # ç§»é™¤ @å¼€å¤´çš„é¢‘é“å
    normalized = re.sub(r'(telegram|channel|proxy|free|v2ray|vpn|config|official|server|test|speed|modeon|nufilter|zedmodeon|rkvps|limproxy|vpnlime|rayx|foxnt|bede|vps)', '', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'(?:[ğŸ‡¦ğŸ‡§ğŸ‡¨ğŸ‡©ğŸ‡ªğŸ‡«ğŸ‡¬ğŸ‡­ğŸ‡®ğŸ‡¯ğŸ‡°ğŸ‡±ğŸ‡²ğŸ‡³ğŸ‡´ğŸ‡µğŸ‡¶ğŸ‡·ğŸ‡¸ğŸ‡¹ğŸ‡ºğŸ‡»ğŸ‡¼ğŸ‡½ğŸ‡¾ğŸ‡¿\U0001F1E6-\U0001F1FF]+)', '', normalized) # ç§»é™¤å›½æ—— emoji

    # ç§»é™¤æ•°å­—å’Œå¯èƒ½çš„åºå· (ä¾‹å¦‚ 1, 01, #1, -1)
    normalized = re.sub(r'(?<![a-zA-Z])\b\d{1,4}\b(?![a-zA-Z])', '', normalized) # ç§»é™¤ç‹¬ç«‹æ•°å­—
    normalized = re.sub(r'#\d+', '', normalized)
    
    # ç§»é™¤æ—¶é—´æˆ³ã€æ—¥æœŸæ ¼å¼
    normalized = re.sub(r'\d{1,2}[-/.]\d{1,2}[-/.]\d{2,4}', '', normalized) # MM-DD-YYYY, DD.MM.YY etc.
    normalized = re.sub(r'\d{4}-\d{2}-\d{2}', '', normalized) # YYYY-MM-DD
    
    # ç§»é™¤å¸¸è§çš„ä¼ è¾“åè®®å’Œå®‰å…¨ç‰¹æ€§
    normalized = re.sub(r'(tls|ssl|tcp|ws|grpc|http|h2|h1\.1|none|reality|xtls|vless|vmess|trojan|ss|ssr|hysteria|hysteria2|hy2|tuic|juicity|socks|naive\+)', '', normalized)
    
    # ç§»é™¤éšæœºå­—ç¬¦ä¸²æˆ–å“ˆå¸Œå€¼ï¼ˆé€šå¸¸æ˜¯8-32ä½å­—æ¯æ•°å­—æ··åˆï¼‰
    normalized = re.sub(r'\b[a-f0-9]{8,32}\b', '', normalized) # ç§»é™¤çœ‹èµ·æ¥åƒhashçš„å­—ç¬¦ä¸²
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™çš„ç©ºæ ¼/è¿å­—ç¬¦/ä¸‹åˆ’çº¿
    normalized = re.sub(r'[^\w\s-]', '', normalized) # åªä¿ç•™å­—æ¯æ•°å­—ï¼Œç©ºæ ¼å’Œè¿å­—ç¬¦
    normalized = re.sub(r'[\s_-]+', ' ', normalized).strip() # å°†å¤šä¸ªç©ºç™½ã€è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºä¸€ä¸ªç©ºæ ¼ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼

    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç‰¹æ®Šæ ‡è®°
    normalized = normalized.strip('+-*/#_')

    # å¦‚æœæ¸…ç†åå­—ç¬¦ä¸²éå¸¸çŸ­ï¼Œå¯èƒ½å¤±å»æ„ä¹‰ï¼Œå¯ä»¥è€ƒè™‘ç›´æ¥è¿”å›ç©ºæˆ–ç‰¹å®šæ ‡è®°
    if len(normalized) < 3: # é¿å…ç©ºå­—ç¬¦ä¸²æˆ–è€…åªæœ‰ä¸€ä¸¤ä¸ªå­—ç¬¦çš„å¤‡æ³¨
        return ''

    return normalized

def generate_node_name(canonical_id_hash):
    """æ ¹æ®è§„èŒƒåŒ– ID çš„å“ˆå¸Œå€¼ç”Ÿæˆä¸€ä¸ªçŸ­èŠ‚ç‚¹åç§°"""
    return hashlib.md5(canonical_id_hash.encode('utf-8')).hexdigest()[:8]

def parse_and_canonicalize(link_string):
    """è§£æã€æ¸…ç†ã€è§£ç å¹¶è§„èŒƒåŒ–ä»£ç†é“¾æ¥"""
    if not link_string or not isinstance(link_string, str):
        logging.debug("è¾“å…¥é“¾æ¥å­—ç¬¦ä¸²ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ç±»å‹ã€‚")
        return None

    cleaned_decoded_link = clean_and_urldecode(link_string)
    if not cleaned_decoded_link:
        logging.debug(f"æ¸…æ´—æˆ– URL è§£ç å¤±è´¥: {link_string[:50]}...")
        return None

    link_parts = cleaned_decoded_link.split('#', 1)
    link_without_fragment = link_parts[0]
    original_remark = link_parts[1] if len(link_parts) > 1 else ''
    scheme = ''
    parsed_payload_for_urlparse = link_without_fragment
    vmess_json_payload = None
    ssr_decoded_parts = None

    if link_without_fragment.startswith('vmess://'):
        scheme = 'vmess'
        try:
            b64_payload = link_without_fragment[8:]
            b64_payload += '=' * (-len(b64_payload) % 4)
            decoded_payload_str = base64.b64decode(b64_payload).decode("utf-8", errors='replace')
            vmess_json_payload = json.loads(decoded_payload_str)
            
            # ä½¿ç”¨ VMess å†…éƒ¨çš„ add å’Œ port æ¥æ„å»ºä¸€ä¸ªä¸´æ—¶çš„ URL ç”¨äº urlparse
            add = vmess_json_payload.get('add', '')
            port = vmess_json_payload.get('port', '')
            
            # è§„èŒƒåŒ– VMess host for temporary URL
            if '@' in add and not is_uuid(add):
                add = add.split('@')[-1]
            add = normalize_domain(add) # ç»Ÿä¸€ host æ ¼å¼
            
            if add and port:
                parsed_payload_for_urlparse = f"vmess://{add}:{port}"
            else:
                logging.debug(f"VMess JSON ç¼ºå°‘ 'add' æˆ– 'port' å­—æ®µ: {decoded_payload_str[:50]}...")
                return None
        except Exception as e:
            logging.debug(f"VMess Base64 è§£ç æˆ– JSON è§£æå¤±è´¥: {link_without_fragment[:50]}... é”™è¯¯: {e}")
            return None

    elif link_without_fragment.startswith('ssr://'):
        scheme = 'ssr'
        try:
            b64_payload = link_without_fragment[6:]
            b64_payload += '=' * (-len(b64_payload) % 4)
            decoded_payload_str = base64.b64decode(b64_payload).decode("utf-8", errors='replace')
            
            # SSR æ ¼å¼ï¼šhost:port:protocol:method:obfs:password_b64/?params
            if '/?' in decoded_payload_str:
                main_part, query_part = decoded_payload_str.split('/?', 1)
            else:
                main_part = decoded_payload_str
                query_part = ''
            
            ssr_decoded_parts = main_part.split(':')
            
            if len(ssr_decoded_parts) >= 6:
                ssr_host = normalize_domain(ssr_decoded_parts[0]) # è§„èŒƒåŒ– SSR host
                ssr_port = ssr_decoded_parts[1]
                
                # é‡æ–°æ„å»ºç”¨äº urlparse çš„ SSR é“¾æ¥ï¼Œç®€åŒ–è·¯å¾„éƒ¨åˆ†
                parsed_payload_for_urlparse = f"ssr://{ssr_host}:{ssr_port}"
                if query_part:
                    parsed_payload_for_urlparse += f"?{query_part}"
            else:
                logging.debug(f"SSR Base64 è§£ç å†…å®¹æ ¼å¼ä¸ç¬¦ (ç¼ºå°‘å¿…éœ€éƒ¨åˆ†): {decoded_payload_str[:50]}...")
                return None
        except Exception as e:
            logging.debug(f"SSR Base64 è§£ç å¤±è´¥æˆ–è§£æé”™è¯¯: {link_without_fragment[:50]}... é”™è¯¯: {e}")
            return None

    elif '://' in link_without_fragment:
        scheme = link_without_fragment.split('://')[0].lower()
    else:
        logging.debug(f"é“¾æ¥ä¸åŒ…å«åè®®: {link_without_fragment[:50]}...")
        return None

    try:
        # å¯¹ URL è¿›è¡Œåˆæ­¥è§£æ
        parsed = urlparse(parsed_payload_for_urlparse)
        if not is_valid_link(parsed):
            logging.debug(f"é“¾æ¥åŸºæœ¬éªŒè¯å¤±è´¥: {link_without_fragment[:50]}...")
            return None

        # è§„èŒƒåŒ– host å’Œ port
        host = normalize_domain(parsed.hostname) # ä½¿ç”¨å¢å¼ºçš„ normalize_domain
        port = parsed.port
        userinfo = parsed.username

        if not scheme or not host or not port:
            logging.debug(f"é“¾æ¥ç¼ºå°‘å¿…éœ€ç»„ä»¶: {link_without_fragment}")
            return None

        canonical_id_components = [scheme]
        ignore_userinfo = os.getenv('IGNORE_USERINFO', 'false').lower() == 'true'
        
        # é’ˆå¯¹ userinfo çš„æ›´ä¸¥æ ¼è§„èŒƒåŒ–
        if userinfo:
            if ignore_userinfo and is_uuid(userinfo):
                logging.debug(f"å¿½ç•¥ UUID userinfo ç”¨äºå»é‡: {userinfo}")
            elif ignore_userinfo and not is_uuid(userinfo):
                # å¦‚æœä¸æ˜¯ UUID ä¸”é…ç½®ä¸ºå¿½ç•¥ userinfoï¼Œåˆ™å¯¹ userinfo è¿›è¡Œè§„èŒƒåŒ–å¤„ç†
                # è¿™ä¼šå¤„ç†å¦‚ TELEGRAM_BYA_Rk_Vps_Rk_Vps è¿™æ ·çš„ userinfo
                normalized_userinfo = normalize_repeated_patterns(userinfo)
                if normalized_userinfo:
                    canonical_id_components.append(normalized_userinfo)
                logging.debug(f"è§„èŒƒåŒ–é UUID userinfo ç”¨äºå»é‡: '{userinfo}' -> '{normalized_userinfo}'")
            elif not ignore_userinfo: # å¦‚æœä¸å¿½ç•¥ userinfo
                if scheme == 'ss':
                    try:
                        # SS userinfo è§£ç åæ˜¯ method:password
                        userinfo_decoded = base64.b64decode(userinfo + '=' * (-len(userinfo) % 4)).decode("utf-8", errors='replace')
                        if ':' in userinfo_decoded:
                            method, password = userinfo_decoded.split(':', 1)
                            canonical_id_components.append(f"{method.lower()}:{password.lower()}") # SS methodå’Œpasswordå°å†™å‚ä¸å»é‡
                        else:
                            logging.debug(f"SS userinfo è§£ç åæ ¼å¼é”™è¯¯: {userinfo_decoded}")
                            return None
                    except Exception as e:
                        logging.debug(f"SS userinfo Base64 è§£ç å¤±è´¥: {userinfo[:50]}... é”™è¯¯: {e}")
                        return None
                else:
                    canonical_id_components.append(userinfo)

        canonical_id_components.append(f"{host}:{port}")

        # è§„èŒƒåŒ– path
        canonical_path = normalize_path(parsed.path) # ä½¿ç”¨å¢å¼ºçš„ normalize_path
        if canonical_path:
            canonical_id_components.append(f"path={quote(canonical_path)}")

        query_params = parse_qs(parsed.query, keep_blank_values=True)
        canonical_query_list = []
        for key in sorted(query_params.keys()):
            key_lower = key.lower()
            
            # ç‰¹æ®Šå¤„ç† host å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨äº query ä¸­ï¼‰
            if key_lower == 'host':
                for value in sorted(query_params[key]):
                    # è§„èŒƒåŒ– host å€¼ï¼Œç§»é™¤ www. å¹¶è½¬å°å†™
                    normalized_value = normalize_domain(value) # ä½¿ç”¨å¢å¼ºçš„ normalize_domain
                    if normalized_value: # åªæœ‰éç©ºæ‰åŠ å…¥
                        canonical_query_list.append(f"{quote(key_lower)}={quote(normalized_value)}")
                continue # å·²å¤„ç†ï¼Œè·³è¿‡åç»­é€šç”¨é€»è¾‘

            # ç‰¹æ®Šå¤„ç† sni å‚æ•°
            if key_lower == 'sni':
                for value in sorted(query_params[key]):
                    normalized_value = normalize_domain(value) # ä½¿ç”¨å¢å¼ºçš„ normalize_domain
                    if normalized_value: # åªæœ‰éç©ºæ‰åŠ å…¥
                        canonical_query_list.append(f"{quote(key_lower)}={quote(normalized_value)}")
                continue # å·²å¤„ç†ï¼Œè·³è¿‡åç»­é€šç”¨é€»è¾‘

            if key_lower in NON_CRITICAL_QUERY_PARAMS:
                logging.debug(f"å¿½ç•¥éå…³é”®æŸ¥è¯¢å‚æ•° '{key_lower}' ç”¨äºå»é‡ã€‚")
                continue

            for value in sorted(query_params[key]):
                # ç‰¹æ®Šå¤„ç† serviceName å‚æ•°
                if key_lower == 'servicename':
                    normalized_value = normalize_repeated_patterns(value)
                    canonical_query_list.append(f"{quote(key_lower)}={quote(normalized_value)}")
                # ç‰¹æ®Šå¤„ç† alpn å‚æ•°ï¼Œè¿›è¡Œæ’åºè§„èŒƒåŒ–
                elif key_lower == 'alpn':
                    sorted_alpn_values = sorted([s.strip() for s in value.split(',') if s.strip()])
                    normalized_value = ','.join(sorted_alpn_values)
                    if normalized_value: # åªæœ‰éç©ºæ‰åŠ å…¥
                        canonical_query_list.append(f"{quote(key_lower)}={quote(normalized_value)}")
                else:
                    canonical_query_list.append(f"{quote(key_lower)}={quote(value)}")
        if canonical_query_list:
            canonical_id_components.append(f"query={';'.join(canonical_query_list)}")

        vmess_params = None
        if scheme == 'vmess' and vmess_json_payload:
            # å¢åŠ äº† 'flow' å‚æ•°ï¼Œè¿™åœ¨ VMess Reality ä¸­å¾ˆé‡è¦
            vmess_fields = ['id', 'aid', 'net', 'type', 'host', 'path', 'tls', 'sni', 'v', 'add', 'port', 'scy', 'fp', 'alpn', 'flow'] 
            vmess_params = {}
            for field in vmess_fields:
                value = vmess_json_payload.get(field)
                if value is not None and value != '':
                    field_lower = field.lower()
                    if field_lower in NON_CRITICAL_QUERY_PARAMS: # å¿½ç•¥ VMess å†…éƒ¨çš„éå…³é”®å‚æ•°
                        logging.debug(f"å¿½ç•¥ VMess å†…éƒ¨éå…³é”®å‚æ•° '{field_lower}' ç”¨äºå»é‡ã€‚")
                        continue

                    # è§„èŒƒåŒ– VMess host (add) å’Œ sni
                    if field_lower in ['add', 'host', 'sni'] and isinstance(value, str):
                        # å¦‚æœ add åŒ…å« @ å¹¶ä¸”ä¸æ˜¯ UUIDï¼Œåˆ™åªå– @ åé¢çš„éƒ¨åˆ†
                        if field_lower == 'add' and '@' in value and not is_uuid(value.split('@')[0]):
                             value = value.split('@')[-1]
                        value = normalize_domain(value) # ç»Ÿä¸€ host/sni æ ¼å¼ï¼Œä½¿ç”¨å¢å¼ºçš„ normalize_domain
                    
                    # è§„èŒƒåŒ– serviceName å’Œ path
                    if field_lower in ['servicename', 'path'] and isinstance(value, str):
                        value = normalize_repeated_patterns(value) # ä½¿ç”¨å¢å¼ºçš„ normalize_repeated_patterns
                    
                    # è§„èŒƒåŒ– alpn
                    if field_lower == 'alpn' and isinstance(value, str):
                        sorted_alpn_values = sorted([s.strip() for s in value.split(',') if s.strip()])
                        value = ','.join(sorted_alpn_values)
                        if not value: # å¦‚æœè§„èŒƒåŒ–åä¸ºç©ºï¼Œåˆ™è·³è¿‡
                            continue
                    
                    vmess_params[field] = str(value).lower()
            if vmess_params:
                vmess_canonical_parts = [f"{k}={quote(v)}" for k, v in sorted(vmess_params.items())]
                canonical_id_components.append(f"vmess_params={';'.join(vmess_canonical_parts)}")

        ssr_params = None
        if scheme == 'ssr' and ssr_decoded_parts:
            try:
                if len(ssr_decoded_parts) >= 6:
                    ssr_protocol = ssr_decoded_parts[2]
                    ssr_method = ssr_decoded_parts[3]
                    ssr_obfs = ssr_decoded_parts[4]
                    password_part_with_query = ssr_decoded_parts[5]
                    
                    actual_password_b64 = password_part_with_query
                    if '/?' in password_part_with_query:
                        actual_password_b64 = password_part_with_query.split('/?')[0]

                    password = base64.b64decode(actual_password_b64 + '=' * (-len(actual_password_b64) % 4)).decode("utf-8", errors='replace')
                    
                    ssr_params = {
                        'protocol': ssr_protocol.lower(),
                        'method': ssr_method.lower(),
                        'obfs': ssr_obfs.lower(),
                        'password': password.lower()
                    }

                    if '?' in link_without_fragment:
                        full_query_string = link_without_fragment.split('?', 1)[1]
                        ssr_custom_params = parse_qs(full_query_string, keep_blank_values=True)
                        for key in sorted(ssr_custom_params.keys()):
                            key_lower = key.lower()
                            if key_lower in NON_CRITICAL_QUERY_PARAMS: # å¿½ç•¥ SSR å†…éƒ¨çš„éå…³é”®å‚æ•°
                                logging.debug(f"å¿½ç•¥ SSR å†…éƒ¨éå…³é”®å‚æ•° '{key_lower}' ç”¨äºå»é‡ã€‚")
                                continue

                            for value in sorted(ssr_custom_params[key]):
                                # è§„èŒƒåŒ– serviceName æˆ–å…¶å®ƒå¯èƒ½é‡å¤çš„å‚æ•°
                                if key_lower == 'servicename':
                                    ssr_params[key_lower] = normalize_repeated_patterns(value) # ä½¿ç”¨å¢å¼ºçš„ normalize_repeated_patterns
                                # è§„èŒƒåŒ– alpn
                                elif key_lower == 'alpn':
                                    sorted_alpn_values = sorted([s.strip() for s in value.split(',') if s.strip()])
                                    normalized_value = ','.join(sorted_alpn_values)
                                    if normalized_value:
                                        ssr_params[key_lower] = normalized_value
                                else:
                                    ssr_params[key_lower] = value

                    ssr_canonical_parts = [f"{k}={quote(v)}" for k, v in sorted(ssr_params.items())]
                    canonical_id_components.append(f"ssr_params={';'.join(ssr_canonical_parts)}")
                else:
                    logging.debug(f"SSR è§„èŒƒåŒ–å¤±è´¥: ssr_decoded_parts é•¿åº¦ä¸è¶³ {len(ssr_decoded_parts)}")
                    return None
            except Exception as e:
                logging.debug(f"SSR è§„èŒƒåŒ–å¤±è´¥: {e}")
                return None
        
        # è§„èŒƒåŒ–å¤‡æ³¨å¹¶æ·»åŠ åˆ°å»é‡é”®ä¸­
        normalized_remark = normalize_remark(original_remark)
        # å¯ä»¥é€‰æ‹©æ˜¯å¦å°†è§„èŒƒåŒ–åçš„å¤‡æ³¨åŠ å…¥å»é‡é”®
        # é»˜è®¤åŠ å…¥ï¼Œå¦‚æœå¸Œæœ›ä¸åŠ å…¥ï¼Œå¯ä»¥å°†ç¯å¢ƒå˜é‡ INCLUDE_REMARK_IN_DEDUP_KEY è®¾ç½®ä¸º false
        include_remark_in_dedup_key = os.getenv('INCLUDE_REMARK_IN_DEDUP_KEY', 'true').lower() == 'true'
        if include_remark_in_dedup_key and normalized_remark:
            # ä½¿ç”¨å¤‡æ³¨çš„å“ˆå¸Œå€¼ï¼Œé¿å…è¿‡é•¿çš„å¤‡æ³¨å¯¼è‡´å»é‡é”®è¿‡é•¿
            remark_hash = hashlib.md5(normalized_remark.encode('utf-8')).hexdigest()[:8]
            canonical_id_components.append(f"remark_hash={remark_hash}")
            logging.debug(f"åŒ…å«è§„èŒƒåŒ–å¤‡æ³¨å“ˆå¸Œ '{remark_hash}' ({normalized_remark}) åˆ°å»é‡é”®ã€‚")
        elif include_remark_in_dedup_key and not normalized_remark:
            # å¦‚æœå¤‡æ³¨ä¸ºç©ºï¼Œä¹Ÿè®°å½•ä¸€ä¸‹ï¼Œé¿å…ä¸ºç©ºçš„å¤‡æ³¨è¢«å¿½ç•¥
            canonical_id_components.append("remark_hash=none")
            logging.debug("å¤‡æ³¨è§„èŒƒåŒ–åä¸ºç©ºï¼Œè®°å½• 'remark_hash=none' åˆ°å»é‡é”®ã€‚")
        else:
            logging.debug("é…ç½®ä¸ºä¸åŒ…å«å¤‡æ³¨åˆ°å»é‡é”®ã€‚")


        canonical_id = "###".join(canonical_id_components).lower()
        simplified_canonical_id = canonical_id
        
        if ignore_userinfo and userinfo and is_uuid(userinfo):
            # ä»…å½“ userinfo æ˜¯ UUID ä¸”å¿½ç•¥ userinfo é€‰é¡¹å¼€å¯æ—¶æ‰è¿›è¡Œæ›¿æ¢
            # æ‰¾åˆ°å¹¶æ›¿æ¢ userinfo éƒ¨åˆ†
            simplified_canonical_id = simplified_canonical_id.replace(f"{userinfo.lower()}###", "")
            logging.debug(f"å»é‡æ—¶å¿½ç•¥ UUID userinfoï¼Œç®€åŒ–åçš„ ID: {simplified_canonical_id}")
        
        if canonical_id:
            node_name = generate_node_name(simplified_canonical_id) # æ ¹æ®ç®€åŒ–åçš„ ID ç”Ÿæˆåç§°
            return {
                'canonical_id': canonical_id,
                'simplified_canonical_id': simplified_canonical_id,
                'link': cleaned_decoded_link,
                'scheme': scheme,
                'host': host, # è¿”å›è§„èŒƒåŒ–åçš„ host
                'port': port,
                'userinfo': userinfo, # åŸå§‹ userinfo
                'path': canonical_path,
                'query': query_params, # åŸå§‹ query_paramsï¼Œç”¨äºä¿å­˜
                'vmess_params': vmess_params if scheme == 'vmess' else None,
                'ssr_params': ssr_params if scheme == 'ssr' else None,
                'remark': original_remark, # åŸå§‹å¤‡æ³¨
                'normalized_remark': normalized_remark, # è§„èŒƒåŒ–åçš„å¤‡æ³¨
                'node_name': node_name # æ–°ç”Ÿæˆçš„èŠ‚ç‚¹åç§°
            }
        else:
            logging.debug(f"æœªèƒ½ç”Ÿæˆè§„èŒƒåŒ– ID: {link_without_fragment[:50]}...")
            return None

    except Exception as e:
        logging.error(f"è§£ææˆ–è§„èŒƒåŒ–é“¾æ¥é”™è¯¯: {link_string[:50]}... é”™è¯¯: {e}")
        return None

def process_link(link_data):
    """å¤„ç†å•ä¸ªé“¾æ¥å¹¶è¿”å›è§„èŒƒåŒ–ç»“æœ"""
    raw_link, channel_name = link_data
    result = parse_and_canonicalize(raw_link)
    if result:
        return result, channel_name
    return None, channel_name

def process(i_url):
    """å¤„ç†å•ä¸ª Telegram é¢‘é“ä»¥æå–ä»£ç†é“¾æ¥"""
    sem_pars.acquire()
    logging.info(f"å¼€å§‹å¤„ç†é¢‘é“: {i_url}")
    html_pages = []
    cur_url = i_url
    found_links_in_channel = False
    selected_user_agent = random.choice(USER_AGENTS)
    headers = {'User-Agent': selected_user_agent}

    try:
        last_datbef = None
        for itter in range(1, pars_dp + 1):
            if itter > 1 and last_datbef is None:
                logging.debug("ä¸Šä¸€é¡µæœªæ‰¾åˆ°ä¸‹ä¸€é¡µæ•°æ®ç‚¹ï¼Œåœæ­¢åˆ†é¡µã€‚")
                break

            page_url = f'https://t.me/s/{cur_url}' if itter == 1 else f'https://t.me/s/{cur_url}?before={last_datbef[0]}'
            logging.debug(f"å°è¯•è·å–é¡µé¢: {page_url}")

            page_fetched = False
            for attempt in range(3):
                try:
                    response = requests.get(page_url, verify=False, timeout=15, headers=headers)
                    response.raise_for_status()
                    html_pages.append(response.text)
                    match = re.search(r'data-before="(\d+)"', response.text)
                    last_datbef = [match.group(1)] if match else None
                    page_fetched = True
                    logging.debug(f"æˆåŠŸè·å–é¡µé¢: {page_url}")
                    break
                except requests.exceptions.RequestException as e:
                    logging.warning(f"è·å– {page_url} å¤±è´¥ (å°è¯• {attempt + 1}/3): {e}")
                    time.sleep(random.uniform(5, 15))

            if not page_fetched:
                logging.error(f"æœªèƒ½è·å– {page_url}ï¼Œè·³è¿‡ã€‚")
                if itter == 1:
                    break

        if not html_pages:
            logging.warning(f"é¢‘é“ {i_url} æœªè·å–åˆ°ä»»ä½•é¡µé¢å†…å®¹ã€‚")
            return

        for page in html_pages:
            soup = BeautifulSoup(page, 'html.parser')
            code_tags = soup.find_all(class_=['tgme_widget_message_text', 'tgme_widget_message_service'])

            for code_tag in code_tags:
                potential_links = code_tag.get_text(separator='\n').split('\n')
                for raw_link in potential_links:
                    if re.search(r"(vmess|vless|ss|trojan|tuic|hysteria|hysteria2|hy2|juicity|nekoray|socks4|socks5|socks|naive\+|ssr):\/", raw_link, re.IGNORECASE):
                        with data_lock:
                            all_potential_links_data.append((raw_link, i_url))
                        found_links_in_channel = True
                        logging.debug(f"åœ¨é¢‘é“ {i_url} ä¸­æ‰¾åˆ°æ½œåœ¨é“¾æ¥: {raw_link[:100]}...")

        if found_links_in_channel:
            logging.info(f"åœ¨é¢‘é“ {i_url} ä¸­æ‰¾åˆ°æ½œåœ¨é“¾æ¥ã€‚")
        else:
            logging.info(f"åœ¨é¢‘é“ {i_url} ä¸­æœªæ‰¾åˆ°æ½œåœ¨é“¾æ¥ã€‚")

    except Exception as e:
        logging.error(f"å¤„ç†é¢‘é“ {i_url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        sem_pars.release()
        logging.info(f"å®Œæˆå¤„ç†é¢‘é“: {i_url}")

if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    log_level_str = os.getenv('LOG_LEVEL', 'INFO').upper()
    log_level = getattr(logging, log_level_str, logging.INFO)
    logging.basicConfig(level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info("è„šæœ¬å¼€å§‹è¿è¡Œã€‚")

    tg_name_json = json_load(TG_CHANNELS_FILE)
    inv_tg_name_json = json_load(INV_TG_CHANNELS_FILE)

    inv_tg_name_json = [x for x in inv_tg_name_json if isinstance(x, str) and len(x) >= 5]
    tg_name_json = [x for x in tg_name_json if isinstance(x, str) and len(x) >= 5]
    inv_tg_name_json = list(set(inv_tg_name_json) - set(tg_name_json))

    thrd_pars = int(os.getenv('THRD_PARS', '128'))
    pars_dp = int(os.getenv('PARS_DP', '1'))
    use_inv_tc = os.getenv('USE_INV_TC', 'n').lower() == 'y'
    ignore_userinfo = os.getenv('IGNORE_USERINFO', 'false').lower() == 'true' # è·å– IGNORE_USERINFO
    include_remark_in_dedup_key = os.getenv('INCLUDE_REMARK_IN_DEDUP_KEY', 'true').lower() == 'true' # è·å– INCLUDE_REMARK_IN_DEDUP_KEY

    sem_pars = threading.Semaphore(thrd_pars)

    logging.info(f'\nå½“å‰é…ç½®:')
    logging.info(f'  å¹¶å‘æŠ“å–çº¿ç¨‹æ•° (THRD_PARS) = {thrd_pars}')
    logging.info(f'  æ¯ä¸ªé¢‘é“æŠ“å–é¡µé¢æ·±åº¦ (PARS_DP) = {pars_dp}')
    logging.info(f'  ä½¿ç”¨æ— æ•ˆé¢‘é“åˆ—è¡¨ (USE_INV_TC) = {use_inv_tc}')
    logging.info(f'  æ—¥å¿—çº§åˆ« (LOG_LEVEL) = {logging.getLevelName(log_level)}')
    logging.info(f'  å¿½ç•¥ UUID userinfo å»é‡ (IGNORE_USERINFO) = {ignore_userinfo}')
    logging.info(f'  è§„èŒƒåŒ–å¤‡æ³¨å¹¶åŒ…å«åˆ°å»é‡é”® (INCLUDE_REMARK_IN_DEDUP_KEY) = {include_remark_in_dedup_key}')


    logging.info(f'\nç°æœ‰é¢‘é“ç»Ÿè®¡:')
    logging.info(f'  {TG_CHANNELS_FILE} ä¸­çš„é¢‘é“æ€»æ•° - {len(tg_name_json)}')
    logging.info(f'  {INV_TG_CHANNELS_FILE} ä¸­çš„é¢‘é“æ€»æ•° - {len(inv_tg_name_json)}')

    logging.info(f'\nä» {CONFIG_TG_TXT_FILE} ä¸­æå–æ–°çš„ TG é¢‘é“å...')
    config_all = []
    if os.path.exists(CONFIG_TG_TXT_FILE):
        try:
            with open(CONFIG_TG_TXT_FILE, "r", encoding="utf-8") as config_all_file:
                config_all = config_all_file.readlines()
            logging.info(f"æˆåŠŸè¯»å– {CONFIG_TG_TXT_FILE}ã€‚")
        except Exception as e:
            logging.error(f"è¯»å– {CONFIG_TG_TXT_FILE} å¤±è´¥: {e}")

    pattern_telegram_user = re.compile(r'(?:https?:\/\/t\.me\/|@|%40|\bt\.me\/)([a-zA-Z0-9_]{5,})', re.IGNORECASE)
    extracted_tg_names = set()

    for config in config_all:
        cleaned_config = clean_and_urldecode(config)
        if not cleaned_config:
            continue

        # å°è¯•ä» Base64 ç¼–ç çš„é“¾æ¥ä¸­æå–é¢‘é“å (vmess, ssr, ss)
        if cleaned_config.startswith(('vmess://', 'ssr://', 'ss://')):
            try:
                b64_part = ''
                if cleaned_config.startswith('vmess://'):
                    b64_part = cleaned_config[8:]
                elif cleaned_config.startswith('ssr://'):
                    b64_part = cleaned_config[6:]
                elif cleaned_config.startswith('ss://'):
                    parsed_ss = urlparse(cleaned_config)
                    # SS é“¾æ¥çš„ username éƒ¨åˆ†æ˜¯ base64 ç¼–ç çš„ method:passwordï¼Œå¯èƒ½åŒ…å«é¢‘é“å
                    b64_part = parsed_ss.username if parsed_ss.username else ''

                if b64_part:
                    # ç¡®ä¿ Base64 å­—ç¬¦ä¸²é•¿åº¦æ˜¯ 4 çš„å€æ•°
                    b64_part += '=' * (-len(b64_part) % 4)
                    decoded_payload = base64.b64decode(b64_part).decode("utf-8", errors='replace')

                    # å°è¯•ä» VMess 'ps' å’Œ 'add' å­—æ®µä¸­æå–é¢‘é“å
                    if cleaned_config.startswith('vmess://') and decoded_payload:
                        try:
                            vmess_data = json.loads(decoded_payload)
                            for field in ['ps', 'add']: # æ£€æŸ¥ 'ps' å’Œ 'add'
                                if field in vmess_data and isinstance(vmess_data[field], str):
                                    matches = pattern_telegram_user.findall(vmess_data[field])
                                    for match in matches:
                                        cleaned_name = match.lower().strip('_')
                                        if len(cleaned_name) >= 5:
                                            extracted_tg_names.add(cleaned_name)
                                            logging.debug(f"ä» VMess '{field}' æå–é¢‘é“å: {cleaned_name}")
                        except json.JSONDecodeError:
                            logging.debug(f"VMess Base64 è§£ç å†…å®¹ä¸æ˜¯æœ‰æ•ˆ JSON: {decoded_payload[:50]}...")
                        except Exception as ex:
                            logging.debug(f"å¤„ç† VMess è§£ç å†…å®¹é”™è¯¯: {ex}")

                    # å°è¯•ä»æ‰€æœ‰ Base64 è§£ç å†…å®¹ä¸­æå–é¢‘é“å
                    matches = pattern_telegram_user.findall(decoded_payload)
                    for match in matches:
                        cleaned_name = match.lower().strip('_')
                        if len(cleaned_name) >= 5:
                            extracted_tg_names.add(cleaned_name)
                            logging.debug(f"ä» Base64 è§£ç å†…å®¹æå–é¢‘é“å: {cleaned_name}")
            except Exception as e:
                logging.debug(f"ä» Base64 æå–é¢‘é“åå¤±è´¥: {e}")

        # å°è¯•ä»åŸå§‹é“¾æ¥å­—ç¬¦ä¸²ä¸­æå–é¢‘é“å
        matches = pattern_telegram_user.findall(cleaned_config)
        for match in matches:
            cleaned_name = match.lower().strip('_')
            if len(cleaned_name) >= 5:
                extracted_tg_names.add(cleaned_name)
                logging.debug(f"ä»é…ç½®æå–é¢‘é“å: {cleaned_name}")

    initial_tg_count = len(tg_name_json)
    tg_name_json = sorted(list(set(tg_name_json).union(extracted_tg_names)))

    logging.info(f'  æ›´æ–°å {TG_CHANNELS_FILE} é¢‘é“æ€»æ•°: {len(tg_name_json)} (æ–°å¢ {len(tg_name_json) - initial_tg_count})')

    json_dump(tg_name_json, TG_CHANNELS_FILE)

    start_time = datetime.now()
    logging.info(f'\nå¼€å§‹çˆ¬å– TG é¢‘é“å¹¶è§£æé…ç½®...')

    channels_to_process = tg_name_json
    if use_inv_tc:
        channels_to_process = sorted(list(set(tg_name_json).union(inv_tg_name_json)))
        logging.info(f'  å¤„ç† {len(channels_to_process)} ä¸ªé¢‘é“ï¼ˆåŒ…å«æ— æ•ˆé¢‘é“ï¼‰ã€‚')
    else:
        channels_to_process = sorted(list(set(tg_name_json) - set(inv_tg_name_json))) # ä»…å¤„ç†æœ‰æ•ˆé¢‘é“
        logging.info(f'  å¤„ç† {len(channels_to_process)} ä¸ªæœ‰æ•ˆé¢‘é“ã€‚')

    threads = []
    for url in channels_to_process:
        thread = threading.Thread(target=process, args=(url,))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    end_time_parsing = datetime.now()
    logging.info(f'\nçˆ¬å–å®Œæˆ - è€—æ—¶ {str(end_time_parsing - start_time).split('.')[0]}')
    logging.info(f'å…±æå–åˆ° {len(all_potential_links_data)} æ¡æ½œåœ¨é“¾æ¥ã€‚')

    logging.info(f'\nå¼€å§‹å»é‡å’Œè§„èŒƒåŒ–é“¾æ¥...')
    unique_configs = {}
    channels_that_worked = set()
    invalid_links_count = 0

    with ThreadPoolExecutor(max_workers=thrd_pars) as executor:
        results = list(executor.map(process_link, all_potential_links_data))

    for i, (result, channel_name) in enumerate(results):
        if result:
            # æ ¹æ® IGNORE_USERINFO ç¯å¢ƒå˜é‡å’Œ userinfo ç±»å‹å†³å®šä½¿ç”¨å“ªä¸ª key è¿›è¡Œå»é‡
            # è¿™é‡Œçš„ç®€åŒ–é€»è¾‘å·²ç»ç§»åŠ¨åˆ° parse_and_canonicalize å†…éƒ¨ï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨ simplified_canonical_id
            canonical_id_for_dedup = result['simplified_canonical_id']
            
            if canonical_id_for_dedup not in unique_configs:
                unique_configs[canonical_id_for_dedup] = result
                channels_that_worked.add(channel_name)
                logging.debug(f"æ·»åŠ å”¯ä¸€é…ç½® (Key: {canonical_id_for_dedup}): {result['link'][:100]}...")
            else:
                logging.debug(f"è·³è¿‡é‡å¤é…ç½® (Key: {canonical_id_for_dedup}): {result['link'][:100]}...")
        else:
            invalid_links_count += 1
            # åŸå§‹é“¾æ¥å¯èƒ½å¾ˆé•¿ï¼Œåªè®°å½•éƒ¨åˆ†
            original_link_snippet = all_potential_links_data[i][0][:100] + "..."
            logging.debug(f"è·³è¿‡æ— æ•ˆé“¾æ¥ (æ¥è‡ªé¢‘é“: {channel_name}): {original_link_snippet}")


    logging.info(f'å»é‡å®Œæˆ - è€—æ—¶ {str(datetime.now() - end_time_parsing).split('.')[0]}')
    logging.info(f'æœ€ç»ˆå¾—åˆ° {len(unique_configs)} æ¡æœ‰æ•ˆé…ç½®ï¼Œè·³è¿‡ {invalid_links_count} æ¡æ— æ•ˆé“¾æ¥ã€‚')

    logging.info(f'\næ›´æ–°é¢‘é“åˆ—è¡¨...')
    new_tg_name_json = sorted(list(channels_that_worked))
    inv_tg_name_json = sorted(list((set(tg_name_json) - channels_that_worked).union(set(inv_tg_name_json))))
    inv_tg_names = [x for x in inv_tg_name_json if isinstance(x, str) and len(x) >= 5]

    json_dump(new_tg_name_json, TG_CHANNELS_FILE)
    json_dump(inv_tg_names, INV_TG_CHANNELS_FILE)

    logging.info(f'  æ›´æ–°å {TG_CHANNELS_FILE} é¢‘é“æ•°: {len(new_tg_name_json)}')
    logging.info(f'  æ›´æ–°å {INV_TG_CHANNELS_FILE} é¢‘é“æ•°: {len(inv_tg_names)}')

    logging.info(f'\nä¿å­˜æœ‰æ•ˆé…ç½®åˆ° {CONFIG_TG_TXT_FILE} å’Œ {CONFIG_TG_YAML_FILE}...')
    processed_codes_list = [config['link'] for config in unique_configs.values()]
    write_lines(processed_codes_list, CONFIG_TG_TXT_FILE)

    yaml_proxies = []
    for config in unique_configs.values():
        proxy = {
            'name': config['node_name'], # ä½¿ç”¨æ–°ç”Ÿæˆçš„çŸ­åç§°
            'scheme': config['scheme'],
            'host': config['host'], # è¿™é‡Œæ˜¯è§„èŒƒåŒ–åçš„ host
            'port': config['port'],
            'userinfo': config['userinfo'] if config['userinfo'] else None,
            'path': config['path'] or None,
            # å°† query å­—å…¸è½¬æ¢ä¸ºæ›´å‹å¥½çš„æ ¼å¼ï¼Œè·³è¿‡ç©ºå€¼
            'query': {k: v[0] if len(v) == 1 else v for k, v in sorted(config['query'].items()) if v} if config['query'] else None,
            'original_link': config['link'],
            'remark': config['remark'] or None, # åŸå§‹å¤‡æ³¨
            'normalized_remark': config['normalized_remark'] or None, # è§„èŒƒåŒ–åçš„å¤‡æ³¨
            'dedup_key': config['simplified_canonical_id']
        }
        if config['vmess_params']:
            # åŒæ ·ï¼Œvmess_params åº”è¯¥åªåŒ…å«éç©ºå€¼
            proxy['vmess_params'] = {k: v for k, v in config['vmess_params'].items() if v}
        if config['ssr_params']:
            # åŒæ ·ï¼Œssr_params åº”è¯¥åªåŒ…å«éç©ºå€¼
            proxy['ssr_params'] = {k: v for k, v in config['ssr_params'].items() if v}
        yaml_proxies.append(proxy)

    yaml_data = {'proxies': yaml_proxies}
    yaml_dump(yaml_data, CONFIG_TG_YAML_FILE)

    end_time_total = datetime.now()
    logging.info(f'\nè„šæœ¬è¿è¡Œå®Œæ¯•ï¼æ€»è€—æ—¶ - {str(end_time_total - start_time).split('.')[0]}')
